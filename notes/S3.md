# S3
---
S3 URL: httts://s3-region_name.amazonaws.com/bucket_name

- S3 is iobject based
- Files can be from 0 bytes to 5 tb
- There is unlimited storage
- Files are stored in buckets

**Data consistency model:**
- Read after write consistency model for PUTs of new Objects
- Eventual Consistency for overwrite PUTs and DELETE (can take some time to propogate)

**S3 is object based. object consist of following**
- **Key**: name of the object
- **value**: data
- **version id**: important for versioning
- **metadata**: data about data
- **subsources**:
	- ACL: access control list
	- Torrent: for torrenting the file

**S3 Guarantees and features**
- 99.99% availibility
- 99.99999999999% durability for s3 information
- Tiered storage available
- Life cycle management
- versioning
- Encryption
- MFA Delete
- Secure your data using ACL and Bucket Policies

Amazon S3 provides both coarse-grained access controls (Amazon S3 Access Control Lists [ACLs]), and fine-grained access controls (Amazon S3 bucket policies, AWS Identity and Access Management [IAM] policies, and query-string authentication).

Amazon S3 ACLs allow you to grant certain coarse-grained permissions: READ, WRITE, or FULL-CONTROL at the object or bucket level




- Secure your data using 
	IAM Policies
	Query String Authentication
	bucket policies: bucket level security
	CORS
	ACL: individual files
		- authenticated-read: owner gets FULL_CONTROL. The Authenticated Users Group Gets REAG access
		- bucket-owner-read: Object owner gets FULL_CONTROL. Bucket owner gets READ access. If you specify this canned ACL when creating a bucket, Amazon S3 ignores it.
		- bucket-owner-full-control: Both the object owner and bucket owner get FULL_CONTROL over the bucket. If you specify this canned ACL when creating a bucket, Amazon S3 ignores it.
		- log-delivery-write: The LogDelivery group gets WRITE and READ_ACP permissions on the bucket.

---

##### CORS
---
- CORS defines a way for client web applications that are loaded in one domain to intract with resources in a different domain.
- to configure CORS, you create an XML document with rules that identify the origins that you will allow to access your bucket.
- you can add upto 100 rules to the configuration.


---------------------------
S3 storage classes
---------------------------

##### S3 Standard:
* 99.99% availability
* 99.99999999999% durability
* stored redundantly across multiple devices in multiple facilities and is designed to sustain the **loss of 2 facility concurrently**.
* AZ: >=3

##### S3 IA (Infrequently Accessed):
* For data that is accesses less frequently, but requirs rapid access when needed.
* Lower fee than S3 but you are charged a retrival fee.
* 99.9% availability
* AZ: >=3

##### S3 one zone IA:
* a lower cost option for infrequent accessed data but do not require the multiple availability zone data resilience.
* 99.5% availability
* AZ: >=1

#### S3 - Intelligent Tiering
Designed to optimize costs by automatically moving data to the most cost-effective access tire, without perfrmance impact or operational overhead.
* AZ: >=3

#### Glacier:
- Very cheap, but is used for archival only.
- Expedited, standard or bulk.
- A standard retrival takes from minutes to hours
* AZ: >=3

#### Glacier deep archive:
- AWS S3 lowest cost storage class
- Used where 12 hours of retrieval time is acceptable
* AZ: >=3

##### Reduced Redundancy Storage (depricated - omitted)
---
- RRS enables customers to reduce their cost by storing non-critical, reproducible data at lower levels of redundancy than AWS S3 standard storage.
- 99.99% durability

Charges:
---
---
- 	storage
- 	requests
- 	storage management (tags, metadata)
- 	data transfer pricing
- 	Cross region replication:
    - Objects in a bucket in one region are replicated to other buckets inside other regions
- 	transfer acceleration:
    - Amazon s3 transfer acceleration enables fast easy and secure transfer of files over long distance between your end user and an s3 bucket.
    - transfer acceleration takes advantage of amazon aws cloudfronts globally distributed edge location, As the data arrives at an edge location, data is routed to amazon s3 over an optimized network path.

##### Glacier Data Model

1. Vault:
	- A vault is a container for storing archives. When you create a vault, you specify a name and select an AWS region where you want to create the vault.
	- Each vault resource has a unique address
	- https://<region-specific endpoint>/<account id>/vaults/<vaultname>
	- you can store unlimited number of archives in a vault.
	- Each AWS account can have up to 1,000 vaults

2. Archives:
	- An archive can be any data such as photo, video or document and is a base unit of storage in AWS Glacier.
	- https://<region-specific endpoint>/<account id>/vaults/<vaultname>/archives/<archive-id>

3. Job:
	- Retrieving an archieve and vault inventory are asynchrnous operations in AWS Glacier in which you first initiate a Job, and then download the job output after Glacier completes the Job.
	- Data retrieval requests are queued and most jobs take about four hours to complete.

4. Notifiaction configuration:
	- you can configure a vault to send notification to SNS topic when job completes.
5. Vaults Locks:
	- You can easily deploy and enforce compliance controls for individual Amazon Glacier vaults with a vault lock policy. You can specify controls such as Write Once Read Many (WORM) in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed.


-------------------------------------

- Buckets are a universal namespace
- By default all the bucket are private
- Upload an object to s3 receives a HTTP 200 code
- s3, s3 IA, S3 reduced reduncency storage
- Amazon S3 is eventually consistent, but offers read-after-write consistency for new object PUTs.
---

Baseline Performance
---
- S3 scales automatically to high request rates, latency 100-200 ms
- Your app can achieve at least 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD req per seconds per prefix in a bucket
- No number of prefix in a bucket
- KMS Limitation: When using SSE-KMS, you may be impacted by KMS Limitation
    - When uploading a file, GEnerateDataKey KMS API is called
    - When downloading a file, Decyrpt KMS API is called
    - Count towards KMS quota per second (5500, 10000,30000 req/s based on regions) so you will be thorttled
- Multi-part upload:
    - Recommended for 100 MB < files and must be used for files > 5GB
    - Parallelize uploads (speed up transfer)
- S3 Transfer acceleration (Upload only)
    - Increase transfer speed by trasnferring file to AWS edge location which will forward the data to the S3 bucket in the target region (via private network which is fast than public network)
    - Compatible with multipart uplaod
- S3 Byte-Range Fethces
    - Parallelize GETs by requesting specific byte ranges
    - Better resilience in case of failure

S3 Select and Glacier Select
---
- Retrieve less data using SQL by performing serverside filtering
- Can filter by rows and columns (simple sql statements)
- Less network transfer, less CPU cost client-side

**ENCRYPTION:**
---
* AT TRANSIT:
	SSL/TLS

* AT REST:
##### Client side encryption: 
			encryption at local system and upload to s3
##### Server side encryption
		- server side encryption with Amazon s3 management keys (SSE-s3)
		- server side encryption with KMS (SSE-KMS)
		- server side encryption with customer provided keys (SSE-C)

--------------------------------------------------------
##### Range GETs
---
* It is possible to download (GET) only a portion of an object in both Amazon S3 and Amazon Glacier by using something called a Range GET.
* Using the Range HTTP header in the GET request or equivalent parameters in one of the SDK wrapper libraries, you specify a range of bytes of the object. This can be useful in dealing with large objects when you have poor connectivity or to download only a known portion of a large Amazon Glacier backup.

----------------------**Cross Region Replicaiton**----------------------
- Replication is Asynchronous
- versioning must be enabled on both source and destination buckets
- Regions must be unique (different)
- files in an existing bucket are not replicated automatically
- you cannot replicated to multiple buckets
- delete markers are NOT replicated
- If turned on in an existing bucket, cross-region replication will only replicate new objects. Existing objects will not be replicated and must be copied to the new bucket via a separate command.

----------------------**Access Logs**----------------------
- For audit purpose, log all access to S3 buckets
- ANy request made to S3 from any account, authorized or not, will be logged into another S3 bucket
- Never store logs in the same bucekt, it will create an infinite recursive loop
- The logged data can be analyzed using data analysis tools such as AWS Athena


----------------------**Life cycle management**----------------------
- moving objects between storage classes
- transition actions: it defines when objects are transitioned to antoher storage class eg. move objects to standard IA class 60 days after creation
- Expiration actions: configure objects to expire (delete) after some time Eg. Access log files can be set to delete after a 365 days or can be used to delete old versions of files (if versioning enabled)
- Rules can be created for a certain prefix (...mp3/*)
- can be used in conjuction with versioning
- can be applied to current version

----------------------**S3 Object Lock and Glacier Vault Lock**----------------------
(Store and cannot delete objects)
- S3 Object Lock
	- Adopt a WORM (Write Once Read Many) model
	- BLock an object version deletion for a specified amount of time
- Glacier Vault Lock
	- Adopt a WORM (Write Once Read Many) model
	- Lock the policy for future edits (can no longer be changed)
	- Helpful for compliance and data retention


##### AWS S3 bucket Restrictions and Limitations:
---

- By default, you can create upto 100 buckets in each of your AWS accounts.
- High-avalibility engineering of Amazon S3 id=s focused on get, put, list, and delete operations.


#### Pre-Signed URL
- URLs that are only valid for a limited time (S3 user security, eg premium content access to users for a certain time)
- Can generate a signed URL using SDK (hardper, for uploads) or CLI (easy, for downloads)
- By default,expiration of 3600 seconds, can change timeout with --expires-in arg
- Users given a pre-signed url inherit the permissions of the person who generated the URL for GET/PUT


##### Q: When accessing file from one bucket (eg image) inside other bucket (eg static website), we keep on getting 404 error. Why?
Enable CORS

#### Q: Your app on EC2 creates images thumbnails after dp are uploaded on S3. These Thumbnails can be easily recreated, and only need to be kept of 45 days. The source images should be able to be immediately retrieved for these 45 days, and afterwards, the user can wait upto 6 hours. How would you desing this?
S3 source images can be on STANDARD, with a lifecycle config to transition them to GLACIER after 45 days
S3 thumbnails can be on ONEZONE_IA with a lifecycle config to expire(delete) them after 45 days


#### Q: A rule in your company states that you should be able to recover your deleted S3 objects immediately for 15 days, although this may happen rarely. After this time, and for upto 365 days, deleted objects should be recoverable within 48 hours. how would you do this?
You need to enable S3 versioning in order to have object versions, so that deleted objects can be hidden with a delete marker and can be recovered. You can transition these non current versions of the object to S3 IA for 15 days. You can transition afterwards these noncurrent versions to Deep Archive after 15 days. 
